{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes on Political Text\n",
    "\n",
    "In this notebook we use Naive Bayes to explore and classify political data. See the `README.md` for full details. You can download the required DB from the shared dropbox or from blackboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Feel free to include your text patterns functions\n",
    "import os\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from string import punctuation\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#from text_functions_solutions import clean_tokenize, get_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "convention_db = sqlite3.connect(r'/Users/summerpurschke/Desktop/ADS/ADS509/mod4/2020_Conventions.db')\n",
    "convention_cur = convention_db.cursor()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Exploratory Naive Bayes\n",
    "\n",
    "We'll first build a NB model on the convention data itself, as a way to understand what words distinguish between the two parties. This is analogous to what we did in the \"Comparing Groups\" class work. First, pull in the text \n",
    "for each party and prepare it for use in Naive Bayes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conventions\n"
     ]
    }
   ],
   "source": [
    "# Execute SQL query to retrieve table names\n",
    "convention_cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "table_names = convention_cur.fetchall()\n",
    "\n",
    "# Print the table names\n",
    "for name in table_names:\n",
    "    print(name[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute SELECT query on the source database\n",
    "query_results = convention_cur.execute(\"SELECT party, text FROM conventions\")\n",
    "\n",
    "convention_data = []\n",
    "for row in query_results:\n",
    "    convention_data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Fold to lowercase \n",
    "# convention_data = [(item[0].lower(), item[1].lower()) for item in convention_data]\n",
    "\n",
    "# ## remove punctuation\n",
    "# punctuation = set(punctuation) # speeds up comparison\n",
    "# for item in convention_data:\n",
    "#     cleaned_item = tuple([element.translate(str.maketrans(\"\", \"\", string.punctuation)) for element in item])\n",
    "#     convention_data.append(cleaned_item)\n",
    "\n",
    "    \n",
    "\n",
    "# ## remove stopwords   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "convention_data = [(party.lower().translate(str.maketrans(\"\", \"\", string.punctuation)), \n",
    "                 text.lower().translate(str.maketrans(\"\", \"\", string.punctuation)))\n",
    "                 for party, text in convention_data]\n",
    "\n",
    "\n",
    "cleaned_data_lower = []\n",
    "for party, text in convention_data:\n",
    "    lower_party = party.lower()\n",
    "    lower_text = text.lower()\n",
    "    cleaned_data_lower.append((lower_party, lower_text))\n",
    "\n",
    "convention_data = cleaned_data_lower\n",
    "\n",
    "cleaned_data_punct = []\n",
    "\n",
    "for party, text in cleaned_data_lower:\n",
    "    no_punct_party = party.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    no_punct_text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    cleaned_data_punct.append((no_punct_party, no_punct_text))\n",
    "    \n",
    "convention_data = cleaned_data_punct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_set = set(stopwords.words('english'))\n",
    "cleaned_data_sw= []\n",
    "\n",
    "for party, text in convention_data:\n",
    "    cleaned_text = \" \".join([word for word in text.split() if word.lower() not in stopwords_set])\n",
    "    cleaned_data_sw.append((party, cleaned_text))\n",
    "\n",
    "convention_data = cleaned_data_sw\n",
    "\n",
    "cleaned_data_lower = []\n",
    "for party, text in convention_data:\n",
    "    lower_party = party.lower()\n",
    "    lower_text = text.lower()\n",
    "    cleaned_data_lower.append((lower_party, lower_text))\n",
    "\n",
    "convention_data = cleaned_data_lower\n",
    "\n",
    "cleaned_data_punct = []\n",
    "for party, text in cleaned_data_lower:\n",
    "    no_punct_party = party.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    no_punct_text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    cleaned_data_punct.append((no_punct_party, no_punct_text))\n",
    "    \n",
    "convention_data = cleaned_data_punct"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some random entries and see if they look right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('democratic', 'inaudible 0024 55 youâ€™re smiling okay'),\n",
       " ('democratic',\n",
       "  'past years americaâ€™s body politics weakened divisions growing deeper antisemitism antilatino antiimmigrant fervor racism charlottesville kkk didnâ€™t even bother wear hoods minnesota life squeezed mr floyd strong body fight virus americaâ€™s divisions weakened donald trump didnâ€™t create initial division division created trump made worse collective strength exercised government effect immune system current federal government dysfunctional competent couldnâ€™t fight virus fact didnâ€™t even see coming european virus infected northeast white house still fixated china virus attacking us months even knew saw failure government tried deny virus tried ignore try politicize failed federal government watched new york get ambushed negligence watched new york suffer learned absolutely nothing'),\n",
       " ('democratic', '1967'),\n",
       " ('republican',\n",
       "  'iâ€™m supporting president trump believes strong america cannot fight endless wars must continue leave blood treasure middle east quagmires flew dover air force base honor two soldiers whose remains coming home afghanistan iâ€™ll never forget evening tell president felt pain families president committed ending war president trump first president generation seek end war rather start one intends end war afghanistan bringing men women home madison wrote â€œno nation preserve freedom midst continuous warfareâ€ iâ€™m proud finally see president agrees compare president trump disastrous record joe biden whoâ€™s consistently called war'),\n",
       " ('republican',\n",
       "  'rapid deployment medicines patient colleague friend would surely died third end july developed fever cough reached testing team received one abbott rapid tests yet another tool quickly approved administration within 15 minutes test came back positive within four hours receiving remdesivir doses followed infusion convalescent plasma physician iâ€™ve seen firsthand breakthroughs saved countless lives patient iâ€™ve benefited expedited therapies made swift action administration president trump truly moved mountains save lives deserves credit thank president trump providing timely access critical diagnostics therapeutics pandemic thank mr president strong leadership challenging times'),\n",
       " ('democratic', 'go joe announcer 012300 oregon'),\n",
       " ('republican',\n",
       "  'veronica sayez put scrubs every day day day went work one new york cityâ€™s busiest hospitals stayed job put long hours done got back neighborhood helped neighbors friends struggling brother william new york city firefighter theyâ€™re emblematic heroes across country theyâ€™re us tonight say earned admiration american people always grateful service care thanks courage compassion american people weâ€™re slowing spread weâ€™re protecting vulnerable weâ€™re saving lives weâ€™re opening america strong foundation president trump poured first three years weâ€™ve already gained back 93 million jobs last three months alone weâ€™re opening america weâ€™re opening americaâ€™s schools iâ€™m proud report wife karen school teacher iâ€™ve married returning classroom next week'),\n",
       " ('democratic', 'bill legislative train wreck'),\n",
       " ('republican',\n",
       "  'hello name maximo alvarez live miami florida far state florida isnâ€™t 90 mile wide blue strip map divides freedom fear divides past future know past iâ€™ll never forget family fled totalitarianism communism first dad spain cuba familyâ€™s donâ€™t run away grace god live american dream greatest blessing ever dad sixth grade education told â€œdonâ€™t lose place never lucky meâ€ maximo alvarez 020714 iâ€™m speaking today familyâ€™s done abandoning rightfully earned thereâ€™s place hide iâ€™m speaking today president trump may always politically correct fact successful businessman average career politician president another family man friend important elected commander chief puts america first keep mind guy running president mostly concerned power yes yes power benefit americans'),\n",
       " ('republican', 'south carolina')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices(convention_data,k=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that looks good, we now need to make our function to turn these into features. In my solution, I wanted to keep the number of features reasonable, so I only used words that occur at least `word_cutoff` times. Here's the code to test that if you want it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a word cutoff of 5, we have 2 as features in the model.\n"
     ]
    }
   ],
   "source": [
    "word_cutoff = 5\n",
    "\n",
    "tokens = [w for t, p in convention_data for w in t.split()]\n",
    "\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "\n",
    "feature_words = set()\n",
    "\n",
    "for word, count in word_dist.items() :\n",
    "    if count > word_cutoff :\n",
    "        feature_words.add(word)\n",
    "        \n",
    "print(f\"With a word cutoff of {word_cutoff}, we have {len(feature_words)} as features in the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_features(text,fw) :\n",
    "    \"\"\"Given some text, this returns a dictionary holding the\n",
    "       feature words.\n",
    "       \n",
    "       Args: \n",
    "            * text: a piece of text in a continuous string. Assumes\n",
    "            text has been cleaned and case folded.\n",
    "            * fw: the *feature words* that we're considering. A word \n",
    "            in `text` must be in fw in order to be returned. This \n",
    "            prevents us from considering very rarely occurring words.\n",
    "        \n",
    "       Returns: \n",
    "            A dictionary with the words in `text` that appear in `fw`. \n",
    "            Words are only counted once. \n",
    "            If `text` were \"quick quick brown fox\" and `fw` = {'quick','fox','jumps'},\n",
    "            then this would return a dictionary of \n",
    "            {'quick' : True,\n",
    "             'fox' :    True}\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Your code here\n",
    "    \n",
    "    ret_dict = dict()\n",
    "    \n",
    "    return(ret_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(feature_words)>0)\n",
    "assert(conv_features(\"donald is the president\",feature_words)==\n",
    "       {'donald':True,'president':True})\n",
    "assert(conv_features(\"people are american in america\",feature_words)==\n",
    "                     {'america':True,'american':True,\"people\":True})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build our feature set. Out of curiosity I did a train/test split to see how accurate the classifier was, but we don't strictly need to since this analysis is exploratory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(conv_features(text,feature_words), party) for (text, party) in convention_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20220507)\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "test_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set, train_set = featuresets[:test_size], featuresets[test_size:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features(25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a little prose here about what you see in the classifier. Anything odd or interesting?\n",
    "\n",
    "### My Observations\n",
    "\n",
    "_Your observations to come._\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Classifying Congressional Tweets\n",
    "\n",
    "In this part we apply the classifer we just built to a set of tweets by people running for congress\n",
    "in 2018. These tweets are stored in the database `congressional_data.db`. That DB is funky, so I'll\n",
    "give you the query I used to pull out the tweets. Note that this DB has some big tables and \n",
    "is unindexed, so the query takes a minute or two to run on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cong_db = sqlite3.connect(r\"/Users/summerpurschke/Desktop/ADS/ADS509/mod4/congressional_data.db\")\n",
    "cong_cur = cong_db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cong_cur.execute(\n",
    "        '''\n",
    "           SELECT DISTINCT \n",
    "                  cd.candidate, \n",
    "                  cd.party,\n",
    "                  tw.tweet_text\n",
    "           FROM candidate_data cd \n",
    "           INNER JOIN tweets tw ON cd.twitter_handle = tw.handle \n",
    "               AND cd.candidate == tw.candidate \n",
    "               AND cd.district == tw.district\n",
    "           WHERE cd.party in ('Republican','Democratic') \n",
    "               AND tw.tweet_text NOT LIKE '%RT%'\n",
    "        ''')\n",
    "\n",
    "results = list(results) # Just to store it, since the query is time consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "tweet_data_sw = []\n",
    "\n",
    "for party, text in tweet_data:\n",
    "    # Decode the byte object using an appropriate encoding\n",
    "    decoded_text = text.decode('utf-8')  # Adjust the encoding if necessary\n",
    "    # Remove stopwords and clean the text\n",
    "    cleaned_text = \" \".join([word for word in decoded_text.split() if word.lower() not in stopwords_set])\n",
    "    tweet_data_sw.append((party, cleaned_text))\n",
    "\n",
    "tweet_data = tweet_data_sw\n",
    "\n",
    "tweet_data_lower = []\n",
    "for party, text in tweet_data:\n",
    "    lower_party = party.lower()\n",
    "    lower_text = text.lower()\n",
    "    tweet_data_lower.append((lower_party, lower_text))\n",
    "\n",
    "tweet_data = tweet_data_lower\n",
    "\n",
    "tweet_data_punct = []\n",
    "for party, text in tweet_data_lower:\n",
    "    no_punct_party = party.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    no_punct_text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    tweet_data_punct.append((no_punct_party, no_punct_text))\n",
    "    \n",
    "tweet_data = tweet_data_punct"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of tweets here. Let's take a random sample and see how our classifer does. I'm guessing it won't be too great given the performance on the convention speeches..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20201014)\n",
    "\n",
    "tweet_data_sample = random.choices(tweet_data,k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('democratic',\n",
       "  'silverbough awesome thrilled join womensmarchonwashington central coast residents  students httpstcobelyaqwamg'),\n",
       " ('democratic', 'congratulations goteamusa ðŸŽ‰ðŸ‡ºðŸ‡¸ httpstcox2di6gen8g'),\n",
       " ('democratic',\n",
       "  'yet speaker ryan still willing endorse donald trump candidate proposed racist policy httpstconlklulbo8j'),\n",
       " ('republican',\n",
       "  'today celebrate anniversary signing nations founding document philadelphiaâ€¦ httpstcoz0imiybfhe'),\n",
       " ('republican',\n",
       "  'one year realdonaldtrump economy booming people want work finding work maga ðŸ‡ºðŸ‡¸'),\n",
       " ('democratic',\n",
       "  'video heard concerns town hall advocated committee today must protecttheaca httpstcoopgumzs03x'),\n",
       " ('democratic',\n",
       "  'aubrey found hermit crab tide pools hanging cabrillonps 4th gradeâ€¦ httpstcourfgkqkj96'),\n",
       " ('democratic',\n",
       "  'definitely do teacher mother 5 trying changetheconversation washington solving problems getting things done tn04  help us take back house people 2018 httpstco0dffkhvdfk haveyoumetmariah php2018 teachers4congress httpstcov5wahvawf4'),\n",
       " ('democratic',\n",
       "  'thank you ltg charles d luckey meeting today ltg luckey chief commanding general usarmyreserve httpstcoplelw3sfqs'),\n",
       " ('democratic',\n",
       "  'investigation ms gÃ³mezâ€™s tragic death continues unfold keep pressing case handled fairness transparency')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_data_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's our (cleaned) tweet: democratic\n",
      "Actual party is silverbough awesome thrilled join womensmarchonwashington central coast residents  students httpstcobelyaqwamg and our classifier says aubrey found hermit crab tide pools hanging cabrillonps 4th gradeâ€¦ httpstcourfgkqkj96.\n",
      "\n",
      "Here's our (cleaned) tweet: democratic\n",
      "Actual party is congratulations goteamusa ðŸŽ‰ðŸ‡ºðŸ‡¸ httpstcox2di6gen8g and our classifier says aubrey found hermit crab tide pools hanging cabrillonps 4th gradeâ€¦ httpstcourfgkqkj96.\n",
      "\n",
      "Here's our (cleaned) tweet: democratic\n",
      "Actual party is yet speaker ryan still willing endorse donald trump candidate proposed racist policy httpstconlklulbo8j and our classifier says aubrey found hermit crab tide pools hanging cabrillonps 4th gradeâ€¦ httpstcourfgkqkj96.\n",
      "\n",
      "Here's our (cleaned) tweet: republican\n",
      "Actual party is today celebrate anniversary signing nations founding document philadelphiaâ€¦ httpstcoz0imiybfhe and our classifier says one year realdonaldtrump economy booming people want work finding work maga ðŸ‡ºðŸ‡¸.\n",
      "\n",
      "Here's our (cleaned) tweet: republican\n",
      "Actual party is one year realdonaldtrump economy booming people want work finding work maga ðŸ‡ºðŸ‡¸ and our classifier says one year realdonaldtrump economy booming people want work finding work maga ðŸ‡ºðŸ‡¸.\n",
      "\n",
      "Here's our (cleaned) tweet: democratic\n",
      "Actual party is video heard concerns town hall advocated committee today must protecttheaca httpstcoopgumzs03x and our classifier says aubrey found hermit crab tide pools hanging cabrillonps 4th gradeâ€¦ httpstcourfgkqkj96.\n",
      "\n",
      "Here's our (cleaned) tweet: democratic\n",
      "Actual party is aubrey found hermit crab tide pools hanging cabrillonps 4th gradeâ€¦ httpstcourfgkqkj96 and our classifier says aubrey found hermit crab tide pools hanging cabrillonps 4th gradeâ€¦ httpstcourfgkqkj96.\n",
      "\n",
      "Here's our (cleaned) tweet: democratic\n",
      "Actual party is definitely do teacher mother 5 trying changetheconversation washington solving problems getting things done tn04  help us take back house people 2018 httpstco0dffkhvdfk haveyoumetmariah php2018 teachers4congress httpstcov5wahvawf4 and our classifier says aubrey found hermit crab tide pools hanging cabrillonps 4th gradeâ€¦ httpstcourfgkqkj96.\n",
      "\n",
      "Here's our (cleaned) tweet: democratic\n",
      "Actual party is thank you ltg charles d luckey meeting today ltg luckey chief commanding general usarmyreserve httpstcoplelw3sfqs and our classifier says aubrey found hermit crab tide pools hanging cabrillonps 4th gradeâ€¦ httpstcourfgkqkj96.\n",
      "\n",
      "Here's our (cleaned) tweet: democratic\n",
      "Actual party is investigation ms gÃ³mezâ€™s tragic death continues unfold keep pressing case handled fairness transparency and our classifier says aubrey found hermit crab tide pools hanging cabrillonps 4th gradeâ€¦ httpstcourfgkqkj96.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a CountVectorizer to convert text into numerical features\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Extract the tweet texts and parties from tweet_data_sample\n",
    "tweets = [tweet for tweet, _ in tweet_data_sample]\n",
    "parties = [party for _, party in tweet_data_sample]\n",
    "\n",
    "# Convert the tweet texts into numerical features\n",
    "X = vectorizer.fit_transform(tweets)\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X, parties)\n",
    "\n",
    "# Predict the party for each tweet and print the results\n",
    "for tweet, party in tweet_data_sample:\n",
    "    tweet_text = tweet\n",
    "    # Convert the tweet text into numerical features\n",
    "    X_tweet = vectorizer.transform([tweet_text])\n",
    "    # Predict the party using the trained classifier\n",
    "    predicted_party = classifier.predict(X_tweet)[0]\n",
    "    \n",
    "    print(f\"Here's our (cleaned) tweet: {tweet_text}\")\n",
    "    print(f\"Actual party is {party} and our classifier says {predicted_party}.\")\n",
    "    print(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've looked at it some, let's score a bunch and see how we're doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of counts by actual party and estimated party. \n",
    "# first key is actual, second is estimated\n",
    "parties = ['Republican','Democratic']\n",
    "results = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for p in parties :\n",
    "    for p1 in parties :\n",
    "        results[p][p1] = 0\n",
    "\n",
    "\n",
    "num_to_score = 10000\n",
    "random.shuffle(tweet_data)\n",
    "\n",
    "for idx, tp in enumerate(tweet_data) :\n",
    "    tweet, party = tp    \n",
    "    # Now do the same thing as above, but we store the results rather\n",
    "    # than printing them. \n",
    "   \n",
    "    # get the estimated party\n",
    "    estimated_party = \"Gotta fill this in\"\n",
    "    \n",
    "    results[party][estimated_party] += 1\n",
    "    \n",
    "    if idx > num_to_score : \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflections\n",
    "\n",
    "_Write a little about what you see in the results_ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
